#!/usr/bin/env python
import gc
import logging
import hydra
from hydra.utils import instantiate
from omegaconf import OmegaConf

from vae_ld.data.util import get_unique_samples
from vae_ld.learning_dynamics.fondue import fondue, fondue_var_type


from vae_ld.learning_dynamics.variable_filter import filter_variables

logger = logging.getLogger("fondue")


@hydra.main(config_path="config", config_name="nb_latents")
def get_optimal_nb_latents(cfg):
    logger.info("Experiment config:\n{}".format(OmegaConf.to_yaml(cfg)))

    logger.info("Instantiating estimator")
    estimator = instantiate(cfg.fondue_estimator)

    logger.info("Instantiating {} dataset".format(cfg.dataset.name))

    sampler = instantiate(cfg.sampling, batch_size=cfg.ide_batch_size)
    X = get_unique_samples(sampler[0][0])
    del sampler
    gc.collect()

    sampler = instantiate(cfg.sampling)
    data_ide = cfg.data_ide

    logger.info("Searching the number of latent dimensions")
    if cfg.fondue_type != "var_type":
        n = fondue(estimator.fit_transform, data_ide, X, sampler, cfg)

    else:
        n = fondue_var_type(filter_variables, data_ide, X, sampler, cfg)
    logger.info("The number of latent dimensions is {}".format(n))


if __name__ == "__main__":
    get_optimal_nb_latents()
