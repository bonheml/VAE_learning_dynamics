#!/usr/bin/env python
import copy
import gc
import glob
import logging
from pathlib import Path
import hydra
import tensorflow as tf
from hydra.utils import instantiate
from omegaconf import OmegaConf
import numpy as np

from vae_ld.data.util import natural_sort

logger = logging.getLogger("train")
OmegaConf.register_new_resolver("len", lambda x: len(x))

@hydra.main(config_path="config", config_name="training")
def train(cfg):
    logger.info("Model config:\n{}".format(OmegaConf.to_yaml(cfg)))
    logger.info("Ensuring Tensorflow and Numpy are seeded...")
    tf.random.set_seed(cfg.seed)
    rng = np.random.RandomState(cfg.seed)

    logger.info("Creating the optimiser...")
    optimizer = instantiate(cfg.optimizer)

    logger.info("Retrieving the data...")
    # We create a partial instantiation first to share the same dataset between samplers
    sampler = instantiate(cfg.sampling, _partial_=True)
    logger.debug("The partial sampler is {}".format(sampler))
    train_sampler = sampler()
    logger.debug("The train sampler is {}".format(train_sampler))
    test_sampler = sampler()
    logger.debug("The test sampler is {}".format(test_sampler))
    # We update the test_sampler with the validation indexes from the train sampler
    test_sampler.validation = True
    test_sampler.validation_idxs = train_sampler.validation_idxs
    # Remove unused index list to avoid unneeded memory usage
    test_sampler.train_idxs = None
    train_sampler.validation_idxs = None

    logger.info("Instantiating callbacks and creating subdirectories for callback logs...")
    callbacks = []
    for k, v in cfg.callbacks.items():
        if k == "image_generator":
            callback_sampler = sampler()
            callback_sampler.batch_size = v.nb_samples
            data_callback = callback_sampler[0][0]
            logger.debug("Add data of shape {} to image_generator".format([d.shape for d in data_callback]
                                                                          if isinstance(data_callback, tuple) else
                                                                          data_callback.shape))
            data_callback = data_callback
            greyscale = cfg.dataset.observation_shape[2] == 1
            callbacks.append(instantiate(v, data=data_callback, greyscale=greyscale))
        else:
            callbacks.append(instantiate(v))
        if "filepath" in v.keys() or "log_dir" in v.keys():
            path = Path(k)
            logger.info("Creating new folder {}".format(path))
            path.mkdir(parents=True, exist_ok=True)
            if k == "checkpoint":
                checkpoint = glob.glob("{}/*".format(path))
                checkpoint.sort(key=natural_sort)
                checkpoint = checkpoint[-1] if checkpoint != [] else None

    steps_per_epochs = train_sampler.data.data_size // cfg.batch_size

    epochs = max(cfg.training_steps // steps_per_epochs, 1)

    logger.info("Creating the model...")
    model = instantiate(cfg.model)
    # Note that run_eagerly must be set to true to allow data generator on custom models
    model.compile(optimizer=optimizer, run_eagerly=True)

    logger.info("Starting model training...")
    hist = model.fit(train_sampler, epochs=epochs, batch_size=cfg.batch_size, callbacks=callbacks,
                     validation_data=test_sampler)
    model.save("checkpoint/epoch_{}_model_loss_{:.2f}".format(epochs, hist.history["model_loss"][-1]))
    model.save("final_model")
    gc.collect()


if __name__ == "__main__":
    train()
