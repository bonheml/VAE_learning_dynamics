#!/usr/bin/env python
import glob
import logging
import re
from pathlib import Path
import hydra
import tensorflow as tf
from hydra.utils import instantiate
from omegaconf import OmegaConf
import numpy as np

from vae_ld.data.util import natural_sort

logger = logging.getLogger("train")


@hydra.main(config_path="config/training", config_name="config")
def train(cfg):
    tf.config.set_visible_devices([], 'GPU')
    logger.info("Model config:\n{}".format(OmegaConf.to_yaml(cfg)))
    logger.info("Ensuring Tensorflow and Numpy are seeded...")
    tf.random.set_seed(cfg.seed)
    random_state = np.random.RandomState(cfg.seed)

    logger.info("Creating the optimiser...")
    optimizer = instantiate(cfg.optimizer)

    logger.info("Retrieving the data...")
    train_sampler = instantiate(cfg.sampling)
    test_sampler = instantiate(cfg.sampling)
    test_sampler.validation = True

    logger.info("Instantiating callbacks and creating subdirectories for callback logs...")
    callbacks = []
    checkpoint = None
    for k, v in cfg.callbacks.items():
        if k == "image_generator":
            data_callback = train_sampler.data.sample(v.nb_samples, random_state)[1]
            callbacks.append(instantiate(v, data=data_callback))
        else:
            callbacks.append(instantiate(v))
        if ("filepath" or "logdir") in v.keys():
            path = Path(k)
            path.mkdir(parents=True, exist_ok=True)
            if k == "checkpoint":
                checkpoint = glob.glob("{}/*".format(path.parent))
                checkpoint = checkpoint.sort(key=natural_sort)[-1] if checkpoint != [] else None

    steps_per_epochs = train_sampler.data.data_size // cfg.batch_size
    epochs = max(cfg.training_steps // steps_per_epochs, 1)

    if checkpoint is None:
        logger.info("Creating the model...")
        model = instantiate(cfg.model)
        # Note that run_eagerly must be set to true to allow data generator on custom models
        model.compile(optimizer=optimizer, run_eagerly=True)
    else:
        logger.info("Loading model from last checkpoint {}...".format(checkpoint))
        model = tf.keras.models.load_model(checkpoint)
        last_epoch = re.match(r"epoch_(\d+)")
        last_epoch = 0 if last_epoch is None else int(last_epoch.group(1))
        logger.info("Restarting at epoch {}/{}...".format(last_epoch, epochs))
        epochs -= last_epoch

    logger.info("Starting model training...")
    model.fit(train_sampler, epochs=epochs, batch_size=cfg.batch_size, callbacks=callbacks, validation_data=test_sampler)
    model.save("checkpoint/final_model")

if __name__ == "__main__":
    train()
