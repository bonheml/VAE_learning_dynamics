#!/usr/bin/env python
import logging
import hydra
import numpy as np
from hydra.utils import instantiate
from omegaconf import OmegaConf
import tensorflow as tf
from vae_ld.models.encoders import Sampling
from vae_ld.visualisation.gmm import plot_hist

logger = logging.getLogger("ivae_latent_histograms")


def get_data(sampler, encoder, prior_variance, prior_mean, n_items, batch_size):
    mu, v, vp, z, vp, mp, zp = None, None, None, None, None, None, None
    sampling = Sampling()
    for i in range(n_items // batch_size):
        X = sampler[i][0]
        mu_tmp, v_tmp, _ = encoder(X)[-3:]
        z_tmp = sampling((mu_tmp, v_tmp)).numpy()
        vp_tmp = prior_variance(X[1]).numpy()
        mp_tmp = prior_mean(X[1]).numpy()
        logger.info(prior_mean.output_shape)
        logger.info(mp_tmp)
        logger.info(prior_mean(X[1]))
        logger.info(prior_mean.predict(X[1]))
        logger.info("mu prior shape: {}, log_var prior shape:{}".format(tf.shape(mp_tmp), tf.shape(vp_tmp)))
        zp_tmp = sampling((mp_tmp, vp_tmp)).numpy()
        mu_tmp, v_tmp = mu_tmp.numpy(), v_tmp.numpy()
        if i == 0:
            mu, v, z, mp, vp, zp = mu_tmp, v_tmp, z_tmp, mp_tmp, vp_tmp, zp_tmp
        else:
            mu, v, z, mp, vp, zp = (np.vstack((mu, mu_tmp)), np.vstack((v, v_tmp)), np.vstack((z, z_tmp)),
                                    np.vstack((mp, mp_tmp)), np.vstack((vp, vp_tmp)), np.vstack((zp, zp_tmp)))
    return np.nan_to_num(np.exp(v)), mu, z, np.nan_to_num(np.exp(vp)), mp, zp


@hydra.main(config_path="config", config_name="ivae_latents")
def plot_histograms(cfg):
    logger.info("Experiment config:\n{}".format(OmegaConf.to_yaml(cfg)))
    logger.info("Retrieving the data...")
    # We create a partial instantiation first to share the same dataset between samplers
    sampler = instantiate(cfg.sampling)
    optimizer = instantiate(cfg.optimizer)

    logger.info("Loading model from {}".format(cfg.model_path))
    model = tf.keras.models.load_model(cfg.model_path)
    encoder = model.encoder
    encoder.trainable = False
    prior_variance = model.prior_variance
    prior_variance.trainable = False
    prior_mean = model.prior_mean
    prior_mean.trainable = False
    model.compile(optimizer)
    model.summary()

    logger.info(prior_mean)

    logger.info("Sampling the data")
    var, mu, z, var_prior, mp, zp = get_data(sampler, encoder, prior_variance, prior_mean, cfg.num_train, cfg.batch_size)

    for i in cfg.var_idx:
        var_i, mu_i, z_i, varp_i, mp_i, zp_i = var[:, i], mu[:, i], z[:, i], var_prior[:, i], mp[:,i], zp[:,i]
        var_i, mu_i, z_i = var_i.reshape(-1, 1), mu_i.reshape(-1, 1), z_i.reshape(-1, 1)
        varp_i, mp_i, zp_i = varp_i.reshape(-1, 1), mp_i.reshape(-1, 1), zp_i.reshape(-1, 1)
        plot_hist(z_i, "{}_z_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{z}$")
        plot_hist(zp_i, "{}_prior_z_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{z^{\dagger}}$")
        plot_hist(var_i, "{}_var_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{\sigma}$")
        plot_hist(varp_i, "{}_prior_var_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{\sigma^{\dagger}}$")
        plot_hist(mu_i, "{}_mean_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{\mu}$")
        plot_hist(mp_i, "{}_prior_mean_{}_histogram.pdf".format(cfg.fname, i), r"$\mathbf{\mu^{\dagger}}$")


if __name__ == "__main__":
    plot_histograms()
