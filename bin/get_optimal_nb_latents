#!/usr/bin/env python
import logging
import hydra
from hydra.utils import instantiate
from omegaconf import OmegaConf

from vae_ld.learning_dynamics.fondue import fondue_ide
from vae_ld.learning_dynamics.utils import prepare_activations
import numpy as np

logger = logging.getLogger("get_optimal_nb_latents")


@hydra.main(config_path="config", config_name="nb_latents")
def get_optimal_nb_latents(cfg):
    logger.info("Experiment config:\n{}".format(OmegaConf.to_yaml(cfg)))

    logger.info("Instantiating intrinsic dimension estimator")
    ide = instantiate(cfg.ide_estimator)

    logger.info("Instantiating {} dataset".format(cfg.dataset.name))

    ide_sampler = instantiate(cfg.sampling, batch_size=cfg.ide_batch_size)
    X = np.unique(ide_sampler[0][0], axis=0)
    del ide_sampler

    sampler = instantiate(cfg.sampling)

    logger.info("Computing data IDE")
    data_ide = int(ide.fit_transform(prepare_activations(X)))

    logger.info("Searching the optimal number of latent dimensions")
    #if cfg.fondue_type == "ide":
    n = fondue_ide(ide.fit_transform, data_ide, X, sampler, cfg)
    # else:
    #    n = fondue_var_type(filter_variables, data_ide, X, sampler, cfg)
    logger.info("The optimal dimensionality for the latent dimensions is {}".format(n))


if __name__ == "__main__":
    get_optimal_nb_latents()
