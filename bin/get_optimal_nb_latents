#!/usr/bin/env python
import logging
import hydra
from hydra.utils import instantiate
from omegaconf import OmegaConf

from vae_ld.learning_dynamics.fondue import fondue
from vae_ld.learning_dynamics.utils import prepare_activations
import numpy as np

logger = logging.getLogger("get_optimal_nb_latents")


@hydra.main(config_path="config", config_name="nb_latents")
def get_optimal_nb_latents(cfg):
    logger.info("Experiment config:\n{}".format(OmegaConf.to_yaml(cfg)))

    logger.info("Instantiating intrinsic dimension estimator")
    ide = instantiate(cfg.ide_estimator)

    logger.info("Instantiating {} dataset".format(cfg.dataset.name))
    ide_sampler = instantiate(cfg.sampling, batch_size=cfg.ide_batch_size)
    X = np.unique(ide_sampler[0][0], axis=0)
    del ide_sampler

    sampler = instantiate(cfg.sampling)

    logger.info("Computing data IDE")
    data_ide = int(ide.fit_transform(prepare_activations(X)))

    logger.info("Searching the optimal number of latent dimensions")
    n = fondue(ide.fit_transform, data_ide, X, sampler, cfg)

    logger.info("The optimal dimensionality for the latent dimensions is {}".format(n))


if __name__ == "__main__":
    get_optimal_nb_latents()
